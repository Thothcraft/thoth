{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e8613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train a classifier on sanitized CSI stored in NPZ files.\n",
    "\n",
    "This notebook mirrors the processing steps in `visualization.ipynb`, but it\n",
    "loads pre-sanitized CSI contained in `.npz` archives written by the SHARP\n",
    "pipeline (see `sanitize.ipynb`).  Each archive is expected to contain at\n",
    "least the key `H` with shape `(N_pkts, K)` (complex) – the sanitized channel\n",
    "frequency response of one capture.\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Repository root (one level up from the notebook directory)\n",
    "REPO_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b1cfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded             drink.csv.npz | class=drink  | samples=42006 | total drink=42006\n",
      "Loaded               eat.csv.npz | class=eat    | samples=39659 | total eat=39659\n",
      "Loaded             empty.csv.npz | class=empty  | samples=38103 | total empty=38103\n",
      "Loaded             sleep.csv.npz | class=sleep  | samples=39998 | total sleep=39998\n",
      "Loaded             smoke.csv.npz | class=smoke  | samples=39676 | total smoke=39676\n",
      "Loaded             watch.csv.npz | class=watch  | samples=38024 | total watch=38024\n",
      "Loaded              work.csv.npz | class=work   | samples=38185 | total work=38185\n",
      "\n",
      "[Summary] Sample distribution per class:\n",
      "  empty : 38103\n",
      "  watch : 38024\n",
      "  work  : 38185\n",
      "  smoke : 39676\n",
      "  eat   : 39659\n",
      "  drink : 42006\n",
      "  sleep : 39998\n",
      "  TOTAL : 275651 samples\n",
      "\n",
      "Loaded            drink2.csv.npz | class=drink  | samples=37661 | total drink=37661\n",
      "Loaded               eat.csv.npz | class=eat    | samples=37980 | total eat=37980\n",
      "Loaded             empty.csv.npz | class=empty  | samples=38724 | total empty=38724\n",
      "Loaded            smoke2.csv.npz | class=smoke  | samples=37795 | total smoke=37795\n",
      "Loaded             watch.csv.npz | class=watch  | samples=38920 | total watch=38920\n",
      "Loaded              work.csv.npz | class=work   | samples=38128 | total work=38128\n",
      "\n",
      "[Summary] Sample distribution per class:\n",
      "  empty : 38724\n",
      "  watch : 38920\n",
      "  work  : 38128\n",
      "  smoke : 37795\n",
      "  eat   : 37980\n",
      "  drink : 37661\n",
      "  sleep : 0\n",
      "  TOTAL : 229208 samples\n",
      "\n",
      "Train set: (275651, 255) Test set: (229208, 255)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 1. Load per-packet features from NPZ files (train / test directories)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "TRAIN_DIR = os.path.join(REPO_ROOT, 'sanitized_data', 'train')\n",
    "TEST_DIR  = os.path.join(REPO_ROOT, 'sanitized_data', 'test')\n",
    "\n",
    "# Activity keywords → numeric labels\n",
    "CLASS_KEYWORDS = ['empty', 'watch', 'work', 'smoke', 'eat', 'drink', 'sleep']\n",
    "CLS_TO_IDX = {kw: i for i, kw in enumerate(CLASS_KEYWORDS)}\n",
    "\n",
    "# Whether to concatenate phase alongside amplitudes\n",
    "INCLUDE_PHASE = False  # ⇐ set True to use (amp, phase)\n",
    "\n",
    "\n",
    "def load_npz_dir(dir_path, include_phase: bool = False, n_samples: int = 1):\n",
    "    \"\"\"Return (X, y) arrays where each *aggregated packet* becomes one sample.\n",
    "\n",
    "    Behaviour controlled by `n_samples`:\n",
    "      • n_samples == 1 (default): identical to original implementation – each\n",
    "        individual packet is a separate sample.\n",
    "      • n_samples  > 1: consecutive `n_samples` packets are concatenated along\n",
    "        the feature dimension to form one larger sample. Any leftover packets\n",
    "        that do not complete a full group are discarded.\n",
    "\n",
    "    X.shape  = (N_groups,  K)      if include_phase=False and n_samples==1\n",
    "                (N_groups, n*K)    if include_phase=False and n_samples>1\n",
    "                (N_groups, 2K)     if include_phase=True and n_samples==1\n",
    "                (N_groups, 2n*K)   if include_phase=True and n_samples>1\n",
    "    y.shape  = (N_groups,)\n",
    "    \"\"\"\n",
    "    if n_samples < 1:\n",
    "        raise ValueError(\"n_samples must be a positive integer\")\n",
    "\n",
    "    per_class_feats = {idx: [] for idx in range(len(CLASS_KEYWORDS))}\n",
    "    class_counts = {idx: 0 for idx in range(len(CLASS_KEYWORDS))}\n",
    "\n",
    "    if not os.path.exists(dir_path):\n",
    "        print(f\"[!] Directory not found: {dir_path}\")\n",
    "        return np.empty((0, 0), dtype=np.float32), np.empty((0,), dtype=np.int64)\n",
    "\n",
    "    npz_files = glob.glob(os.path.join(dir_path, '*.npz'))\n",
    "    if not npz_files:\n",
    "        print(f\"[!] No NPZ files in {dir_path}\")\n",
    "\n",
    "    for file in sorted(npz_files):\n",
    "        fname = os.path.basename(file)\n",
    "        label_idx = next((CLS_TO_IDX[k] for k in CLASS_KEYWORDS if k in fname), None)\n",
    "        if label_idx is None:\n",
    "            print(f\"[skip] Unrecognised class in filename: {fname}\")\n",
    "            continue\n",
    "\n",
    "        data = np.load(file)\n",
    "        H = data['H']  # shape: (N_pkts, K) complex\n",
    "        amp = np.abs(H).astype(np.float32)  # (N_pkts, K)\n",
    "        if include_phase:\n",
    "            phase = np.angle(H).astype(np.float32)  # (N_pkts, K)\n",
    "            feat_mat = np.concatenate([amp, phase], axis=1)  # (N_pkts, 2K)\n",
    "        else:\n",
    "            feat_mat = amp\n",
    "\n",
    "        # Optionally concatenate `n_samples` consecutive packets\n",
    "        if n_samples > 1:\n",
    "            total_pkts = feat_mat.shape[0]\n",
    "            groups = total_pkts // n_samples\n",
    "            if groups == 0:\n",
    "                print(f\"[skip] Not enough packets in {fname} (need ≥{n_samples})\")\n",
    "                continue\n",
    "            trimmed = feat_mat[:groups * n_samples]\n",
    "            feat_mat = trimmed.reshape(groups, -1)  # (groups, n_samples * D)\n",
    "\n",
    "        # Stack features for this label\n",
    "        per_class_feats[label_idx].append(feat_mat)\n",
    "        class_counts[label_idx] += feat_mat.shape[0]\n",
    "\n",
    "        print(f\"Loaded {fname:>25} | class={CLASS_KEYWORDS[label_idx]:<6} | samples={feat_mat.shape[0]:>5} | total {CLASS_KEYWORDS[label_idx]}={class_counts[label_idx]}\")\n",
    "\n",
    "    # Build final arrays\n",
    "    feats_all, labels_all = [], []\n",
    "    for idx, feat_list in per_class_feats.items():\n",
    "        if feat_list:\n",
    "            stacked = np.vstack(feat_list)\n",
    "            feats_all.append(stacked)\n",
    "            labels_all.append(np.full(stacked.shape[0], idx, dtype=np.int64))\n",
    "\n",
    "    if not feats_all:\n",
    "        return np.empty((0, 0), dtype=np.float32), np.empty((0,), dtype=np.int64)\n",
    "\n",
    "    X = np.vstack(feats_all)\n",
    "    y = np.concatenate(labels_all)\n",
    "\n",
    "    # Summary distribution\n",
    "    print(\"\\n[Summary] Sample distribution per class:\")\n",
    "    for idx, name in enumerate(CLASS_KEYWORDS):\n",
    "        print(f\"  {name:<6}: {class_counts[idx]}\")\n",
    "    print(f\"  TOTAL : {len(y)} samples\\n\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Default call – keep original behaviour (per-packet samples)\n",
    "X_train_raw, y_train = load_npz_dir(TRAIN_DIR, include_phase=INCLUDE_PHASE, n_samples=5)\n",
    "X_test_raw , y_test  = load_npz_dir(TEST_DIR , include_phase=INCLUDE_PHASE, n_samples=5)\n",
    "\n",
    "print('Train set:', X_train_raw.shape, 'Test set:', X_test_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbf23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded             drink.csv.npz | class=drink  | samples=   210 | total drink=210\n",
      "Loaded               eat.csv.npz | class=eat    | samples=   198 | total eat=198\n",
      "Loaded             empty.csv.npz | class=empty  | samples=   190 | total empty=190\n",
      "Loaded             sleep.csv.npz | class=sleep  | samples=   199 | total sleep=199\n",
      "Loaded             smoke.csv.npz | class=smoke  | samples=   198 | total smoke=198\n",
      "Loaded             watch.csv.npz | class=watch  | samples=   190 | total watch=190\n",
      "Loaded              work.csv.npz | class=work   | samples=   190 | total work=190\n",
      "\n",
      "[Summary] Sample distribution per class:\n",
      "  empty : 190\n",
      "  watch : 190\n",
      "  work  : 190\n",
      "  smoke : 198\n",
      "  eat   : 198\n",
      "  drink : 210\n",
      "  sleep : 199\n",
      "  TOTAL : 1375 samples\n",
      "\n",
      "Performed internal split: train=962 test=413\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 1. Load aggregated packet features from NPZ files (train directory)\n",
    "#    Optionally split into train/test (70/30) internally.\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "TRAIN_DIR = os.path.join(REPO_ROOT, 'sanitized_data', 'train')\n",
    "TEST_DIR  = os.path.join(REPO_ROOT, 'sanitized_data', 'test2')  # kept for backward-compat\n",
    "\n",
    "# Activity keywords → numeric labels\n",
    "CLASS_KEYWORDS = ['empty', 'watch', 'work', 'smoke', 'eat', 'drink', 'sleep']\n",
    "CLS_TO_IDX = {kw: i for i, kw in enumerate(CLASS_KEYWORDS)}\n",
    "\n",
    "# Whether to concatenate phase alongside amplitudes\n",
    "INCLUDE_PHASE = False  # ⇐ set True to use (amp, phase)\n",
    "\n",
    "\n",
    "def load_npz_dir(\n",
    "    dir_path: str,\n",
    "    *,\n",
    "    include_phase: bool = False,\n",
    "    n_samples: int = 1,\n",
    "    split: bool = False,\n",
    "    split_ratio: float = 0.3,\n",
    "    random_state: int = 0,\n",
    "):\n",
    "    \"\"\"Load CSI features from a directory of ``*.npz`` files.\n",
    "\n",
    "    Returns *per-sample* feature matrix **X** and label vector **y**.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_path : str\n",
    "        Folder containing ``*.npz`` archives (each with key ``H``).\n",
    "    include_phase : bool, default False\n",
    "        If ``True``, concatenate amplitude & phase; otherwise amplitude only.\n",
    "    n_samples : int, default 1\n",
    "        Concatenate this many consecutive packets to form one sample.  If\n",
    "        leftovers < ``n_samples`` remain, they are discarded.\n",
    "    split : bool, default False\n",
    "        If ``True``, **ignores any separate test directory** and instead\n",
    "        returns an *internal 70/30 train/test split* of the data loaded from\n",
    "        *dir_path*.\n",
    "    split_ratio : float, default 0.3\n",
    "        Proportion of samples to assign to the *test* split when\n",
    "        ``split=True``.\n",
    "    random_state : int, default 0\n",
    "        Seed for shuffling before splitting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, np.ndarray            # if ``split=False``\n",
    "    np.ndarray, np.ndarray,\n",
    "    np.ndarray, np.ndarray            # if ``split=True`` (train_X, train_y, test_X, test_y)\n",
    "    \"\"\"\n",
    "    if n_samples < 1:\n",
    "        raise ValueError(\"n_samples must be a positive integer\")\n",
    "    if not 0 < split_ratio < 1:\n",
    "        raise ValueError(\"split_ratio must be in the open interval (0, 1)\")\n",
    "\n",
    "    per_class_feats = {idx: [] for idx in range(len(CLASS_KEYWORDS))}\n",
    "    class_counts = {idx: 0 for idx in range(len(CLASS_KEYWORDS))}\n",
    "\n",
    "    if not os.path.exists(dir_path):\n",
    "        print(f\"[!] Directory not found: {dir_path}\")\n",
    "        return np.empty((0, 0), dtype=np.float32), np.empty((0,), dtype=np.int64)\n",
    "\n",
    "    npz_files = glob.glob(os.path.join(dir_path, '*.npz'))\n",
    "    if not npz_files:\n",
    "        print(f\"[!] No NPZ files in {dir_path}\")\n",
    "\n",
    "    # --------------------- per-file processing ---------------------\n",
    "    for file in sorted(npz_files):\n",
    "        fname = os.path.basename(file)\n",
    "        label_idx = next((CLS_TO_IDX[k] for k in CLASS_KEYWORDS if k in fname), None)\n",
    "        if label_idx is None:\n",
    "            print(f\"[skip] Unrecognised class in filename: {fname}\")\n",
    "            continue\n",
    "\n",
    "        data = np.load(file)\n",
    "        H = data['H']                        # (N_pkts, K) complex\n",
    "        amp = np.abs(H).astype(np.float32)   # (N_pkts, K)\n",
    "        if include_phase:\n",
    "            phase = np.angle(H).astype(np.float32)    # (N_pkts, K)\n",
    "            feat_mat = np.concatenate([amp, phase], axis=1)  # (N_pkts, 2K)\n",
    "        else:\n",
    "            feat_mat = amp\n",
    "\n",
    "        # --- optional packet aggregation ---\n",
    "        if n_samples > 1:\n",
    "            total_pkts = feat_mat.shape[0]\n",
    "            groups = total_pkts // n_samples          # floor division – discard leftovers\n",
    "            if groups == 0:\n",
    "                print(f\"[skip] Not enough packets in {fname} (need ≥{n_samples})\")\n",
    "                continue\n",
    "            trimmed = feat_mat[:groups * n_samples]    # (groups*n, D)\n",
    "            feat_mat = trimmed.reshape(groups, -1)      # (groups, n*D)\n",
    "\n",
    "        per_class_feats[label_idx].append(feat_mat)\n",
    "        class_counts[label_idx] += feat_mat.shape[0]\n",
    "\n",
    "        print(\n",
    "            f\"Loaded {fname:>25} | class={CLASS_KEYWORDS[label_idx]:<6} | \"\n",
    "            f\"samples={feat_mat.shape[0]:>6} | total {CLASS_KEYWORDS[label_idx]}=\"\n",
    "            f\"{class_counts[label_idx]}\"\n",
    "        )\n",
    "\n",
    "    # --------------------- aggregate all classes ------------------\n",
    "    feats_all, labels_all = [], []\n",
    "    for idx, feat_list in per_class_feats.items():\n",
    "        if feat_list:\n",
    "            stacked = np.vstack(feat_list)\n",
    "            feats_all.append(stacked)\n",
    "            labels_all.append(np.full(stacked.shape[0], idx, dtype=np.int64))\n",
    "\n",
    "    if not feats_all:\n",
    "        return np.empty((0, 0), dtype=np.float32), np.empty((0,), dtype=np.int64)\n",
    "\n",
    "    X = np.vstack(feats_all)\n",
    "    y = np.concatenate(labels_all)\n",
    "\n",
    "    print(\"\\n[Summary] Sample distribution per class:\")\n",
    "    for idx, name in enumerate(CLASS_KEYWORDS):\n",
    "        print(f\"  {name:<6}: {class_counts[idx]}\")\n",
    "    print(f\"  TOTAL : {len(y)} samples\\n\")\n",
    "\n",
    "    # --------------------- optional 70/30 split -------------------\n",
    "    if split:\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        perm = rng.permutation(len(y))\n",
    "        X, y = X[perm], y[perm]\n",
    "        split_idx = int(len(y) * (1 - split_ratio))   # 70% train by default\n",
    "        X_train, y_train = X[:split_idx], y[:split_idx]\n",
    "        X_test,  y_test  = X[split_idx:], y[split_idx:]\n",
    "        print(f\"Performed internal split: train={len(y_train)} test={len(y_test)}\")\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ------------------------- example usage -------------------------\n",
    "# 1️⃣ Original behaviour (full directory → X, y)\n",
    "# X_all, y_all = load_npz_dir(TRAIN_DIR, include_phase=INCLUDE_PHASE)\n",
    "\n",
    "# 2️⃣ Packet aggregation + internal 70/30 split\n",
    "X_tr, y_tr, X_te, y_te = load_npz_dir(\n",
    "    TRAIN_DIR,\n",
    "    include_phase=INCLUDE_PHASE,\n",
    "    n_samples=1000,\n",
    "    split=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e91b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 2. PCA visualisation helper + run\n",
    "# ----------------------------------\n",
    "\n",
    "from typing import Sequence\n",
    "\n",
    "# --- Scale features (standardisation) ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test  = scaler.transform(X_test_raw)\n",
    "\n",
    "\n",
    "def plot_pca_scatter(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: Sequence[int],\n",
    "    X_test: np.ndarray,\n",
    "    y_test: Sequence[int],\n",
    "    class_names: Sequence[str],\n",
    "    n_components: int = 2,\n",
    "):\n",
    "    \"\"\"Visualise train/test sets with 2-D or 3-D PCA scatter plots.\"\"\"\n",
    "    if n_components not in (2, 3):\n",
    "        raise ValueError(\"n_components must be 2 or 3\")\n",
    "\n",
    "    pca = PCA(n_components=n_components, random_state=0)\n",
    "    train_pca = pca.fit_transform(X_train)\n",
    "    test_pca  = pca.transform(X_test)\n",
    "\n",
    "    lbl_train = [class_names[i] for i in y_train]\n",
    "    lbl_test  = [class_names[i] for i in y_test]\n",
    "\n",
    "    if n_components == 2:\n",
    "        fig_tr = px.scatter(x=train_pca[:, 0], y=train_pca[:, 1],\n",
    "                            color=lbl_train, title=\"PCA 2-D – training set\",\n",
    "                            labels={'color': 'class'})\n",
    "        fig_te = px.scatter(x=test_pca[:, 0], y=test_pca[:, 1],\n",
    "                            color=lbl_test, title=\"PCA 2-D – test set\",\n",
    "                            labels={'color': 'class'})\n",
    "    else:\n",
    "        fig_tr = px.scatter_3d(x=train_pca[:, 0], y=train_pca[:, 1], z=train_pca[:, 2],\n",
    "                               color=lbl_train, title=\"PCA 3-D – training set\",\n",
    "                               labels={'color': 'class'})\n",
    "        fig_te = px.scatter_3d(x=test_pca[:, 0], y=test_pca[:, 1], z=test_pca[:, 2],\n",
    "                               color=lbl_test, title=\"PCA 3-D – test set\",\n",
    "                               labels={'color': 'class'})\n",
    "\n",
    "    fig_tr.show(); fig_te.show()\n",
    "\n",
    "# --- Example calls ---\n",
    "# plot_pca_scatter(X_train, y_train, X_test, y_test, CLASS_KEYWORDS, n_components=2)\n",
    "# plot_pca_scatter(X_train, y_train, X_test, y_test, CLASS_KEYWORDS, n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 3. PyTorch classifier\n",
    "# ----------------------\n",
    "\n",
    "class WiFiSensingNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_sizes=(512, 256, 128, 64), dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers += [nn.Linear(prev, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "valid_ds = TensorDataset(X_test_t,  y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=32)\n",
    "\n",
    "model = WiFiSensingNet(X_train.shape[1], len(CLASS_KEYWORDS)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    # ---- training ----\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ---- validation ----\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in valid_loader:\n",
    "                xb = xb.to(device)\n",
    "                out = model(xb)\n",
    "                preds.extend(out.argmax(1).cpu().numpy())\n",
    "                gts.extend(yb.numpy())\n",
    "        acc = accuracy_score(gts, preds)\n",
    "        print(f'Epoch {epoch+1:02d} – test accuracy: {acc*100:.2f}%')\n",
    "\n",
    "print('\\nClassification report (test set):')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_t.to(device)).argmax(1).cpu().numpy()\n",
    "print(classification_report(y_test, preds, target_names=CLASS_KEYWORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28603cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 4. Classical ML baselines: SVC and KNN classifiers\n",
    "# -------------------------------------------------\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# --- Support Vector Classifier ---\n",
    "svc = SVC(kernel='rbf', C=10, gamma='scale')\n",
    "svc.fit(X_train, y_train)\n",
    "svc_pred = svc.predict(X_test)\n",
    "print(\"\\nSVC accuracy: {:.2f}%\".format(accuracy_score(y_test, svc_pred) * 100))\n",
    "print(\"SVC classification report:\\n\", classification_report(y_test, svc_pred, target_names=CLASS_KEYWORDS))\n",
    "\n",
    "# --- k-Nearest Neighbours ---\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "print(\"\\nKNN accuracy: {:.2f}%\".format(accuracy_score(y_test, knn_pred) * 100))\n",
    "print(\"KNN classification report:\\n\", classification_report(y_test, knn_pred, target_names=CLASS_KEYWORDS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
